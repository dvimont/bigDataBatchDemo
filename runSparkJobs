#!/bin/bash
STORAGE_LEVEL=$1 # MEMDISK or DISK
EXECUTOR_MEM=$2  # 1g, 2g, 4g, etc. -- note that r4.2xlarge machines have 60GB ram
SPARK_MASTER=$3 # note that port is usually 7077
HDFS_MASTER=$4
YEAR=$5  # normally entered as "2016" for full year or "2016/10" for testing of one month
OUTPUT_DIR=/output
DAILY_OUTPUT=$OUTPUT_DIR/pageviews.daily/$YEAR
WEEKLY_OUTPUT=$OUTPUT_DIR/pageviews.weekly/$YEAR # weekly processing discontinued
MONTHLY_OUTPUT=$OUTPUT_DIR/pageviews.monthly/$YEAR
YEARLY_OUTPUT=$OUTPUT_DIR/pageviews.yearly/$YEAR

echo "Removing existing HDFS pageview spark output"
hadoop fs -rm -r $DAILY_OUTPUT
hadoop fs -rm -r $MONTHLY_OUTPUT
hadoop fs -rm -r $YEARLY_OUTPUT
hadoop fs -rm -r /debug/

echo "STARTING PageViews Spark processing: " `date -u --rfc-822`

echo "Running SparkDriver job: "  `date -u --rfc-822`
# NOTE: this is customized from an example execution in Learning Spark book
$SPARK_HOME/bin/spark-submit \
  --master $SPARK_MASTER \
  --executor-memory $EXECUTOR_MEM \
  --class org.commonvox.bigdatademos.SparkDriver \
  ./target/bigdatademo01-1.0-SNAPSHOT.jar \
  $HDFS_MASTER $STORAGE_LEVEL \
  /nodelete/pageviews.hourly/$YEAR/* \
  $DAILY_OUTPUT $WEEKLY_OUTPUT \
  $MONTHLY_OUTPUT $YEARLY_OUTPUT  \
    > ./logs/spark_output.log  2>&1

 # args after .jar above == hdfs-url, storage-level, input HDFS file/directory,
 #                            & output daily, weekly, monthly, yearly HDFS files

echo "COMPLETED PageViews Spark processing: " `date -u --rfc-822`

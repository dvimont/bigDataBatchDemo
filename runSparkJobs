#!/bin/bash
NUM_PARTITIONS=$1

echo "Removing existing HDFS pageview mapreduce output"
hadoop fs -rm -r /test/pageviews.*

echo "STARTING PageViews Spark processing: " `date -u --rfc-822`

echo "Running SparkDriver job: "  `date -u --rfc-822`
# NOTE: this is customized from an example execution in Learning Spark book
$SPARK_HOME/bin/spark-submit \
  --master spark://ec2-54-164-189-32.compute-1.amazonaws.com \
  --class org.commonvox.bigdatademos.SparkDriverToRiak \
  ./target/bigdatademo01-1.0-SNAPSHOT.jar \
  hdfs://ec2-54-164-189-32.compute-1.amazonaws.com:9000/ \
  $NUM_PARTITIONS test/raw_files test/pageviews.daily test/pageviews.weekly \
  test/pageviews.monthly test/pageviews.yearly > ./logs/spark_output.log  2>&1
  # args after .jar above == hdfs-url, input HDFS file, nbr-partitions, & output daily, weekly, monthly, yearly HDFS files

echo "COMPLETED PageViews Spark processing: " `date -u --rfc-822`
cd ..

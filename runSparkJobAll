#!/bin/bash
STORAGE_LEVEL=$1 # MEMDISK or DISK
EXECUTOR_MEM=$2  # 1g, 2g, 4g, etc. -- note that r4.2xlarge machines have 60GB ram
SPARK_MASTER=$3 # note that port is usually 7077
HDFS_MASTER=$4
OUTPUT_DIR=/output
DAILY_OUTPUT=$OUTPUT_DIR/pageviews.daily
WEEKLY_OUTPUT=$OUTPUT_DIR/pageviews.weekly # weekly processing discontinued
MONTHLY_OUTPUT=$OUTPUT_DIR/pageviews.monthly
YEARLY_OUTPUT=$OUTPUT_DIR/pageviews.yearly

echo "Removing existing HDFS pageview spark output"
hadoop fs -rm -r $DAILY_OUTPUT
hadoop fs -rm -r $MONTHLY_OUTPUT
hadoop fs -rm -r $YEARLY_OUTPUT

echo "STARTING PageViews Spark processing: " `date -u --rfc-822`

echo "Running SparkDriver job: "  `date -u --rfc-822`
# NOTE: this is customized from an example execution in Learning Spark book
$SPARK_HOME/bin/spark-submit \
  --master $SPARK_MASTER \
  --executor-memory $EXECUTOR_MEM \
  --class org.commonvox.bigdatademos.SparkDriver \
  ./target/bigdatademo01-1.0-SNAPSHOT.jar \
  $HDFS_MASTER $STORAGE_LEVEL \
  /nodelete/pageviews.hourly/* \
  $DAILY_OUTPUT $WEEKLY_OUTPUT \
  $MONTHLY_OUTPUT $YEARLY_OUTPUT  \
    > ./logs/spark_output.log  2>&1

 # args after .jar above == hdfs-url, storage-level, input HDFS file/directory,
 #                            & output daily, weekly, monthly, yearly HDFS files

echo "COMPLETED PageViews Spark processing: " `date -u --rfc-822`
